{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mwolinsky/Datasets/blob/main/NLP_UdeSA_2025_1_a_Tokenizaci%C3%B3n%2C_BoW_y_clasificaci%C3%B3n.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z5SnzbnHRDQ"
      },
      "source": [
        "\n",
        "# Tokenización, bag-of-words y clasificación de textos\n",
        "## Procesamiento del Lenguaje Natural\n",
        "## Maestría en Cs. de Datos, Universidad de San Andrés\n",
        "### 2025\n",
        "### Profesores: Juan Manuel Pérez y Bruno Bianchi\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qybs9OK8IxoL"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers scikit-learn unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8_JZqpRX0Ym"
      },
      "source": [
        "# Tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4LnR2cPX0Ym"
      },
      "source": [
        "Vamos a tokenizar un texto en palabras. Para esto vamos a usar la librería `spacy`, que es una librería popular de NLP con múltiples modelos pre-entrenados para una variedad de tareas.\n",
        "\n",
        "Primero, bajamos el modelo en español e inglés:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlqVOWJOX0Yn"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d5UsJz2X0Yn"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "text = \"Estamos cursando la materia Procesamiento del Lenguaje Natural, dictada por el Dr. Pérez y el Dr. Bianchi en la Universidad de San Andrés\"\n",
        "\n",
        "doc = nlp(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R27iKLBX0Yp"
      },
      "source": [
        "Un objeto de spacy es un \"iterable\" que contiene tokens e información por cada uno de estos tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsuxxT3hX0Yp"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f\"{'Token':<20} {'POS':<20} {'Is Alpha':<20} {'Lemma':<20} {'Is Stop':<20}\")\n",
        "print(\"-\"*140)\n",
        "for token in doc:\n",
        "\n",
        "    print(f\"{token.text:<20} {token.pos_:<20} {token.is_alpha:<20} {token.lemma_:<20} { token.is_stop:<20}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxPXHFivX0Yq"
      },
      "source": [
        "Algunas definiciones rápidas:\n",
        "\n",
        "- **Token**: Un token es una unidad mínima de un texto. Puede ser una palabra, un número, un signo de puntuación, etc.\n",
        "- **POS**: Part of speech. La categoría gramatical de una palabra. Por ejemplo, \"perro\" es un sustantivo, \"corre\" es un verbo, \"rápidamente\" es un adverbio, etc.\n",
        "- **Alpha**: ¿el token es alfanumérico?\n",
        "- **Lema**: La forma base de una palabra. Por ejemplo, el lemma de \"corriendo\" es \"correr\". Usualmente es un verbo en infinito. (en desuso en NLP moderno)\n",
        "- **Stopword**: Palabras que \"no aportan información\" (en un modelo bag-of-words...). Por ejemplo, \"el\", \"la\", \"un\", \"una\", etc. (en desuso en NLP moderno)\n",
        "\n",
        "Las últimas tres columnas suelen ser útiles para preprocesar texto o reducir tamaño de vocabularios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD1Iga0sX0Yq"
      },
      "source": [
        "**Pregunta**: ¿qué tokens les llaman la atención? ¿hubieran separado distinto el texto?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JedY6G5X0Yr"
      },
      "source": [
        "## Cálculo de un vocabulario\n",
        "\n",
        "Recordemos que, dado un corpus de texto, el vocabulario es el conjunto de palabras únicas que aparecen en el texto.\n",
        "\n",
        "Vamos a calcular un vocabulario de un conjunto de oraciones sencillo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX1wDtMsX0Yr"
      },
      "outputs": [],
      "source": [
        "textos = [\n",
        "    \"bolsa de palabras\",\n",
        "    \"palabras de bolsa\",\n",
        "    \"bolsa es una palabra\",\n",
        "    \"palabra es una bolsa\",\n",
        "    \"palabra no es una bolsa\",\n",
        "    \"bolsa es una bolsa\",\n",
        "    \"bolsa es una bolsa y es una palabra\",\n",
        "    \"bolsa bolsa bolsa palabra es es una una y\",\n",
        "]\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Tokeniza un texto\n",
        "    \"\"\"\n",
        "    return [token.text for token in nlp(text)]\n",
        "\n",
        "tokenize(textos[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJB4PwjUX0Yr"
      },
      "outputs": [],
      "source": [
        "def calcular_vocabulario(textos):\n",
        "    # Uso set que es un conjunto (en el sentido matemático de la palabra) de elementos\n",
        "    vocabulario = set()\n",
        "\n",
        "    for texto in textos:\n",
        "        tokens = tokenize(texto)\n",
        "        for token in tokens:\n",
        "            vocabulario.add(token)\n",
        "\n",
        "    # Lo convertimos a una lista primero ¿por qué?\n",
        "    return list(vocabulario)\n",
        "\n",
        "calcular_vocabulario(textos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm9XPSSnX0Ys"
      },
      "source": [
        "Si tenemos un texto, podemos convertirlo a una lista de identificadores de tokens (esto va a ser particularmente útil para modelos neuronales)\n",
        "\n",
        "Para programar esto, usamos dos diccionarios:\n",
        "\n",
        "- itos: \"int-to-string\". Un diccionario que mapea un número a una palabra\n",
        "- stoi: \"string-to-int\". Un diccionario que mapea una palabra a un número\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8SBfDulX0Ys"
      },
      "outputs": [],
      "source": [
        "itos = dict(enumerate(calcular_vocabulario(textos)))\n",
        "# Diccionario por comprensión :-), magia ninja de python\n",
        "stoi = {v: k for k, v in itos.items()}\n",
        "\n",
        "itos, stoi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6Rie09rX0Yt"
      },
      "source": [
        "Ahora, hagamos una función que convierta un texto a una lista de identificadores de tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpQuupR0X0Yt"
      },
      "outputs": [],
      "source": [
        "def to_idx(texto, UNK_IDX=-1):\n",
        "    \"\"\"\n",
        "    Convierte un texto a una lista de índices\n",
        "    \"\"\"\n",
        "    tokens = tokenize(texto)\n",
        "\n",
        "    ret = []\n",
        "\n",
        "    for token in tokens:\n",
        "        ret.append(\n",
        "            stoi.get(token, UNK_IDX)\n",
        "        )\n",
        "\n",
        "    return ret\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1YCpvfXX0Yt"
      },
      "source": [
        "En los casos de que aparezca una palabra que no está en el vocabulario, la función debería devolver un token especial, usualmente conocido como token desconocido (o UNK)\n",
        "\n",
        "Decimos que tenemos en ese caso una palabra OOV (out-of-vocabulary)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlnV6c8zX0Yu"
      },
      "source": [
        "### Ejercicio\n",
        "\n",
        "Correr la función en ejemplos de oraciones sencillas. Probar con palabras que no estén en el vocabulario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mAFtyq0X0Yu"
      },
      "outputs": [],
      "source": [
        "to_idx(\"bolsa bolsa bolsas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPoapFNkX0Yu"
      },
      "source": [
        "## Modelo Bag-of-words\n",
        "\n",
        "Dado un vocabulario de tamaño $V$, el modelo bag-of-words representa cada texto $T$ como la cantidad de veces que aparece cada palabra en el vocabulario en $T$, ignorando el orden de las palabras.\n",
        "\n",
        "**Ejercicio**\n",
        "\n",
        "Dado el vocabulario anterior, implementar la función bag_of_words que recibe un texto y devuelve un vector de tamaño `len(itos)` con la cantidad de veces que aparece cada palabra en el texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRpsvuKXX0Yv"
      },
      "outputs": [],
      "source": [
        "def bag_of_words(texto):\n",
        "    \"\"\"\n",
        "    TODO: Implementar la función bag of words\n",
        "    \"\"\"\n",
        "\n",
        "    # Inicializamos un vector de ceros\n",
        "    bow = [0] * len(itos)\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    pass\n",
        "    ### END SOLUTION\n",
        "    return bow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhBmmFOmXBud"
      },
      "source": [
        "Probar con algunos ejemplos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLZkWngoDkZv"
      },
      "source": [
        "Afortunadamente, podemos usar `sklearn` y su módulo `feature_extraction.text` para hacer esto de manera más sencilla.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snj7mTFDywYt"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "CountVectorizer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsqPXBR18MpE"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "textos = [\n",
        "    \"bolsa de palabras\",\n",
        "    \"palabras de bolsa\",\n",
        "    \"bolsa es una palabra\",\n",
        "    \"palabra es una bolsa\",\n",
        "    \"palabra no es una bolsa\",\n",
        "    \"bolsa es una bolsa\",\n",
        "    \"bolsa es una bolsa y es una palabra\",\n",
        "    \"bolsa bolsa bolsa palabra es es una una y\",\n",
        "]\n",
        "\n",
        "vect = CountVectorizer()\n",
        "\n",
        "vect.fit(textos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57ODvK94HRDl"
      },
      "source": [
        "En `vect.vocabulary_` podemos encontrar el vocabulario construido por `CountVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0iHb8KoHRDn"
      },
      "outputs": [],
      "source": [
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v55GUgnoDy_f"
      },
      "source": [
        "`vect.transform` convierte un conjunto de textos en su representación matricial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHqVETv-HRDk"
      },
      "outputs": [],
      "source": [
        "vect.transform(textos).todense()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v14ApzaHHRDp"
      },
      "source": [
        "Armemos un dataframe para que nos muestre qué significa cada columna de la matriz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tq_t3g1jHRDr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Doy vuelta y ordeno el vocabulario (perdón por lo horrendo)\n",
        "vocab = {v:k for k, v in vect.vocabulary_.items()}\n",
        "columns = [vocab[k] for k in range(len(vocab))]\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    vect.transform(textos).todense(),\n",
        "    columns=columns\n",
        ")\n",
        "\n",
        "df[\"text\"] = textos\n",
        "\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1ZEsC0vEBA4"
      },
      "source": [
        "Este modelo se llama de \"unigramas\" porque sólo considera apariciones de una única palabra. ¿Podemos extender esto?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTVRs8-2HRDu"
      },
      "source": [
        "## n-gramas\n",
        "\n",
        "Una forma de agregar cierto orden  es mediante el uso de n-gramas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3KPlsQiHRDv"
      },
      "outputs": [],
      "source": [
        "\n",
        "vect = CountVectorizer(ngram_range=(1, 2))\n",
        "\n",
        "vect.fit(textos)\n",
        "\n",
        "# Doy vuelta y ordeno el vocabulario (perdón por lo horrendo)\n",
        "vocab = {v:k for k, v in vect.vocabulary_.items()}\n",
        "columns = [vocab[k] for k in range(len(vocab))]\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    vect.transform(textos).todense(),\n",
        "    columns=columns\n",
        ")\n",
        "\n",
        "df[\"text\"] = textos\n",
        "\n",
        "\n",
        "pd.options.display.max_columns = 50\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC-vxvvQW15d"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIhQlnlvHRDx"
      },
      "source": [
        "## Análisis de Sentimientos\n",
        "\n",
        "\n",
        "Carguemos un dataset de Sentiment Analysis con la librería `datasets`\n",
        "\n",
        "¿Qué es `huggingface/datasets`? Un inmenso repositorio de datasets, originalmente de NLP pero que ahora contiene datos de todo tipo. Ya no es necesario andar buscando csvs ni jsons por lugares oscuros de Internet: con la api de `datasets` ya estamos.\n",
        "\n",
        "Pueden ver qué tiene en https://huggingface.co/datasets, así como también su documentación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgsQNFKaE-xY"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"tweet_eval\", \"sentiment\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4nYLXBnX0Yz"
      },
      "source": [
        "Un `Dataset` es un objeto que contiene un conjunto de datos con una estructura similar a un dataframe, aunque algo más restringido. Esto se debe principalmente a que está optimizado para usar grandes volúmenes de datos mediante el tipo de datos `parquet`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-9dwD7IHRDz"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZjpC7fQHRD0"
      },
      "source": [
        "¿Qué son los valores de \"label\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uLsLoLOXwQU"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"].features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSYqhglFEr5V"
      },
      "source": [
        "0 es negativo, 1 neutral, y 2 positivo.\n",
        "\n",
        "Usemos la función [`.map`](https://huggingface.co/docs/datasets/process#map) de datasets que nos ayuda a aplicar un procesado sobre los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLJgPqCeE_oY"
      },
      "outputs": [],
      "source": [
        "label_map = dataset[\"train\"].features[\"label\"].names\n",
        "\n",
        "label_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFjYlq5GFB0I"
      },
      "source": [
        "`.map` toma una función, y nos pasa uno a uno cada elemento de nuestro dataset. Si devolvemos un diccionario con columnas `c1, ... cn`, nos devuelve otro dataset con los valores de dichas columnas que devolvimos para cada elemento del dataset.\n",
        "\n",
        "(es parecido al `.apply` de pandas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWv8ux6LHRD2"
      },
      "outputs": [],
      "source": [
        "# Convirtamos todo a strings...\n",
        "# Cuando usemos pytorch no deberíamos hacer esto :-)\n",
        "dataset = dataset.map(lambda ex: {\"sentiment\": label_map[ex[\"label\"]] })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zmBAiReHRD3"
      },
      "source": [
        "A ver cómo quedó..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOvRDVAlG8Gp"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkJZxsJKFjDO"
      },
      "outputs": [],
      "source": [
        "# Para acceder a un \"ejemplo\"/elemento, hay que pedir el split y luego mandar el índice\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx7KYxch4R7P"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][120]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koKd8o7VHRD4"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][1001]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRUITFveHRD7"
      },
      "source": [
        "## Preprocesamiento\n",
        "\n",
        "Hagamos un preprocesamiento muy básico: convertimos todo a minúsculas y convertimos repeticiones de 3 letras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyRNjQV8HRD8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "re.sub(r\"(.)\\1{3,}\", r\"\\1\\1\\1\", \"aaaaaaahhhhh no te la puedo CREEER\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgtxwWD2HONE"
      },
      "source": [
        "Para preprocesar, usamos de nuevo `.map`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oXTpaCpHReU"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_tweet(example):\n",
        "    text = example[\"text\"].lower()\n",
        "    text = re.sub(r\"(.)\\1{3,}\", r\"\\1\\1\", text)\n",
        "    return {\"text\": text}\n",
        "\n",
        "preprocessed_dataset = dataset.map(preprocess_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztNmbeFEHjXu"
      },
      "outputs": [],
      "source": [
        "preprocessed_dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcYi12DJX0Y_"
      },
      "source": [
        "Convirtamos a pandas y la seguimos ahí. Promo sólo válida para esta notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRc-rMpiHRD9"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_df = preprocessed_dataset[\"train\"].to_pandas()\n",
        "dev_df = preprocessed_dataset[\"validation\"].to_pandas()\n",
        "test_df = preprocessed_dataset[\"test\"].to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjpm_HYRHw8p"
      },
      "source": [
        "## A clasificar se ha dicho!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuO-Ii_rx-qH"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    max_features=30_000,\n",
        "    ngram_range=(1, 2),\n",
        ")\n",
        "\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"vectorizer\", vectorizer),\n",
        "    (\"classifier\", classifier)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "pipe.fit(train_df[\"text\"], train_df[\"sentiment\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNs9KwffMBWl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = pipe.predict(dev_df[\"text\"])\n",
        "y_true = dev_df[\"sentiment\"]\n",
        "\n",
        "print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nj-vdQkgHREA"
      },
      "source": [
        "Veamos algunos errores..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aBn6t757CNB"
      },
      "outputs": [],
      "source": [
        "dev_df[\"pred\"] = y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1J-nGv0yHREA"
      },
      "outputs": [],
      "source": [
        "dev_df.loc[y_true != y_pred].sample(10, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYLMcon1HREB"
      },
      "source": [
        "## Ejercicio 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHm9GjlKHREB"
      },
      "source": [
        "Mejorar el preprocesado del modelo anterior\n",
        "\n",
        "* Usemos un tokenizador especial para Tweets\n",
        "* Reomover hashtags\n",
        "* Sacar palabras muy frecuentes\n",
        "* Convertir números a un token especial\n",
        "\n",
        "Usar `nltk.tokenize.TweetTokenizer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpZWHv1OHREC"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "TweetTokenizer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9iSHuAiHRED"
      },
      "source": [
        "## Ejercicio 2\n",
        "\n",
        "Sacar palabras muy frecuentes y muy infrecuentes del vectorizador. Usar para ello `max_df` y `min_df`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70PxQhhbHREE"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "vectorizer = CountVectorizer(\n",
        "    max_features=30_000,\n",
        "    ngram_range=(1, 2),\n",
        "    #Todo: completar argumentos faltantes\n",
        "    tokenizer=TweetTokenizer(reduce_len=True, strip_handles=True).tokenize\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"vectorizer\", vectorizer),\n",
        "    (\"classifier\", LogisticRegression())\n",
        "])\n",
        "\n",
        "pipe.fit(train_df[\"text\"], train_df[\"label\"])\n",
        "\n",
        "\n",
        "y_pred = pipe.predict(dev_df[\"text\"])\n",
        "y_true = dev_df[\"label\"]\n",
        "\n",
        "print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x-1Yk6AHREH"
      },
      "source": [
        "## Ejercicio 3\n",
        "\n",
        "¿Para qué sirve el parámetro `binary` de `CountVectorizer`?\n",
        "\n",
        "Probar si la utilización de ese parámetro mejora la performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXnRW0VCHREH"
      },
      "source": [
        "## Ejercicio 4\n",
        "\n",
        "Evaluar en test los distintos algoritmos de clasificación\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU2CXfV_Mpla"
      },
      "source": [
        "## Ejercicio 4\n",
        "\n",
        "Ver la [tarea CoLA](https://huggingface.co/datasets/glue/viewer/cola/train) del benchmark GLUE. Describir brevemente qué tipo de tarea es y qué busca hacer.\n",
        "\n",
        "¿Por qué funcionan mal los clasificadores propuestos en esta tarea?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgo2K38VrPcm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"glue\", \"cola\")\n",
        "\n",
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7mQuZW1JBj6"
      },
      "source": [
        "## Ejercicio 5\n",
        "\n",
        "a. Dado un vocabulario de tamaño $V$, ¿cuántos posibles n-gramas de tamaño $n$ podemos formar?\n",
        "\n",
        "b. ¿Qué pasa cuando un n-grama aparece en el conjunto de entrenamiento pero no en el de test? ¿y al revés?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts0KAG3oXBur"
      },
      "source": [
        "## Ejercicio 6.\n",
        "\n",
        "Describa situaciones en las que el modelo bag-of-words puede fallar. ¿Qué tipo de información se pierde al usar este modelo?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "nlp-udesa-2024",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}